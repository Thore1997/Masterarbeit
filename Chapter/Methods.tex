\section{Methods for detecting outliers}
There are plentyfull of methods to detect outliers. Each with advantages and disadvantages.
The most popular methods could be categorized as followes: statistical-based, distance-based, density-based,  clustering-based,
graph-based, ensemble-based and learning-based.\citep{wang2019progress} 
In this thesis, I will examine 3 different methods. A statistical, a clustering-based and a learning-based Method.
Statistical methods or distribution-based methods, consider a data point as an outlier, if it extremly deviates from its distribution.
Statisticians used tools like Box-plots, mean-variance or regression models to detect extreme data points \citep[p.3]{smiti2020critical}.

In this section I will begin describing the robust Mahalanobis Distance and explain why it is adventageous to use the mininum covariance determinant.
Afterwards I will transition to the FAST-MCD Methods, because that will be used in my thesis.
In section 2.2 I will describe the k-nearest neighbor algorithm.
After that I will explain a expansion of k-NN.
In Section 2.3 I will define the newly published Intercont method, wich is based on contranstive learning.

\subsection{Mahalanobis Distance}
The Mahalanobis Distance is a measurment in multidimensional space for the distance between a vector and its distribution mean.
It considers the correlation between dimensions not like for example Euclidean distance that just considers two different points, but ignores correlation between variables. 
But uncorrelated variables are further away from its distribution mean than highly correlated variables. 
By considering the correlation between variables, it is effective for outlier detection in multidimensional datasets, because 
observations that dont have extreme values in single variables, can be flagged as an outlier when considering all variable in combination. 

The Mahalanobis distance $D_M$ is defined as: 

\begin{equation}
D_M(\mathbf{x}, \mathbf{\mu}) = \sqrt{ (\mathbf{x} - \mathbf{\mu})^T \mathbf{\Sigma}^{-1} (\mathbf{x} - \mathbf{\mu}) }
\label{eq:mahalanobis}
\end{equation}

where as $x$ is the vector of the obervation, $\mu$ is the mean vector and $\Sigma$ is the covariance matrix. 
By multiplying the distance between the vektor and its mean and the inverse of the covariance matrix, we standardize the distances to their correlations.
Resulting in a high Mahalanobis distance for uncorrelated vaiables and and a low $M_D$ for highly correlated variables.

The Mahalanobis distance is scale-invariant. In the appendix is a formal proof.
It is not affected by the different scales of features because it measures the distance in units of standard deviations.
So changes in feature units can be made without influencing the resulting distance and there is no need to manually tranform features with different
units to ensure they contribute equally to the distance calculation.

By assuming, that the dataset follows a multivariante normal distribution, you also can assume, that the squared Mahalanobis distance is following a $X^2$ distribution.
You can construct a parametic $X^2$ test to spot outliers with a given significance level ($alpha$). 
Therefor you determine the degrees of freedom, choose a significance level and find the critical $X^2$ value.
If the Mahalanobis distance is bigger than the critical vlue, you can call this observation with a given percentage of uncertainty an outlier. 

Unfortuantely the Mahalanobis distance suffers from a masking effect.
And by simply testing the biggest Mahalanobis distance for outiers, you could get false negative results.
Because it is not robust against outliers itselve.

Both input variables, sample mean and covariance matrix, are highly influenced by outliers.
If a outlier occurs, the sample mean will be draged into the direction of the outlier.
Resulting in an artificial scaled down outlier. 
The covariance matrix describes the shape and orientation of the distribution.
The inflating effect of an outlier, streches the ellipsiod in the direction of the ourlier. 
Resulting in an smaller values in the matrix, because you use the inverse.

To solve the masking effect, plenty of robust Mahalanobis distance calculation were proposed in the literatur.
An estimator is called robust, if an outlier just has a small or no effect on the resulting estimators . 
To asses the robustness of an estimator, \citet{hampel1971general} proposed the breakdown point.
Given a dataset $X$ of size $n$, and an examined location estimator $T_n$, the breakdown value is define as:

\begin{equation}
\epsilon_{n}^{*}(T_{n}, X_{n}) = \frac{1}{n} \min \left\{ m : \sup_{X_{n},{m}} \left\| T_{n}(X_{n,m}) - T_{n}(X_{n}) \right\| = +\infty \right\}
\end{equation}

where as $m$ is the minimum number of observations that need to be replaced with outliers, $X_m$ is the contaminated dataset where $m$ points from $X$ have been replaced by arbitrary values.
It measures the robustness by determing the smallest fraction of outlier needed, for the estimator to take on arbitrary values.
$epsilon_n^*$ is the smallest fraction that causes this unbounded change. 
The maximum value of $epsilon_n^*$ is 0.4 because if the estimator reaches beyond this boundary, it would imply that the estimator would prefer the uncontaminated smaller portion 
of the dataset. If the contaminted data reaches the majority of the dataset, no estimator can distinguish the true underlying distribution from the contaminated one. 
The result could be wrong, because the estimator could have chosen the contaminated dataset as the core dataset.


\subsubsection*{Minimum Covariance Determinant}

Used for mahalanobis distance the breakdown point would be 0, because it takes just one outlier for the variance matrix and/or mean vector to take on arbitraty values. 
To solve this problem, I will use the minimum covariance determinant (MCD). 
The goal of MCD is to find the subset $H$ of $h$ observations in a given dataset with $n$ observations and $p$ variables that minimizes the determinant of the subset's sample covariance matrix.
The determinant $det(A)$ is a scalar of a sqaure matrix $A$.
It determines among other things  the change in orientation and volume.
The absolut value of the determinant the factor of wich the volume changes by multiply the determinant with its matrix $A$.
If the determinant is negative, additionally to changes in volume, the orientation changes as well.


MCD is affine equivariant. That means that MCD estimations of the mean and covariance will follow accordingly to 
affine transformations. For example in a change of measurement, the data will be roted, translated or rescaled but the resulting 
mean and covariance will transform correctly and not stay the same.
Thats important for outlier detection, because estimated outliers will stay outlier even affine transformation took place. \citet{hubert2018minimum}


MCD estimators can only be calculated when the number of subset observations ($h$) exceeds the number of variables ($p$), becaus if not the determinant 
of the resulting covariance matrix will be 0. \citet{hubert2018minimum} recommends at least five observations per dimension to avaid excessiv noise. 
The subsets $h$ size is lays between $\frac{n}{2} \le h \le n$. Clearly it is not possible for subsets observations $h$ to expand the number of total observations $n$. 
The difference between $n$ and $h$ are the added outliers.
The lower bound of $h$ is set to be at 50\% because of the so called majority rule.
If the majority of a dataset contains added outliers, the estimator can't distinguish between inliers data structure and outlier data structure. \todo{cite source}

The MCD estimator is the most robust when 

\begin{equation}
h = \lfloor \frac{n+p+1}{2} \rfloor \approx \frac{n}{2}
\end{equation}

wich corresponds to 0.5 at the population level. But the robusteness is been paid by efficency. 
If the dataset does not contain more than 25\% of outliers, than it is suitable to choose $h  0.75n$. \citep[p.217]{rousseeuw1999fast}
The newly calculated estimator is less robust, but more efficient then before. 

The resulting $\hat{\mu}_{\text{MCD}}$ is the mean vector of the remaining subset $h$. 
And t


The mean an covariance will be transformed accordingly.
This means that for any nonsingular p × p matrix A and
any p-dimensional column vector b it holds that

Therefore, transforming an h-subset with lowest determinant yields an h-subset XHA0 with lowest determinant among all
h-subsets of the transformed data set XA0, and its covariance matrix is transformed appropriately
Finally, we note that the robust mahalanobis distances are affine invariant, meaning they stay the same after transforming the data, which implies that the weighted
estimator is affine equivariant too.

To compute the MCD estimator one has to evaluate $\binom{n}{h}$ of all subsets size $h$.
Therefor \citet{rousseeuw1999fast} proposed an algorithn to increase calculation efficency. 
The algorithm of the concentration steps has the following steps: 

\begin{enumerate}
\item Initialize by drawing a random number of random subsamples of size $p+1$ and calculate the sample meand and the sample covariance matrix
\item Concentrate the subsamples by iteratively moving to a subsample with an smaller covariance matrix.
Begin with the smallest 
 
\item 
\end{enumerate}

Even recently reaseach has een done for new extension for MCD.
Because FAST-MCD begins by drawing random subsets, multiple runs of FastMCD on the same dataset may yield different results (a problem often mitigated by fixing the random seed in implementations). 
Furthermore, it requires drawing many initial subsets to ensure at least one is free of outliers, which can be computationally intensive. 
To circumvent both of these problems, the Deterministic MCD (DetMCD) algorithm was developed. \citep[p.621 pp.]{hubert2012deterministic}
DetMCD provides a deterministic approach to robust location and scatter. 
It uses the same iterative steps as FastMCD but avoids starting with random subsets, ensuring consistency. 
Unlike its predecessor, DetMCD is permutation invariant, meaning its result is independent of the order of observations in the dataset. 
Moreover, DetMCD typically runs even faster than FastMCD and is less sensitive to data contamination (outliers).









\subsection{k-Nearest-Neighbor}
The k-nearest neighbor (kNN) algorithm is a nonparametic classification algorithm. It is been known for its simplicity and accuracy.
It is a supervised learning algorithm, that means, that a labled traning dataset is been used to train the model and afterwards a class of unlabled data can be classified.
It is a "lazy learning" algorithm that means it does not achieve generalizations. The entire traningset is being used for traning.
When a new unlabled data is presented into the dataset, the k-NN algorithm determines in wich class the unlabled data should go by determining the wich classes the neighboring belong to.
Outlier detection is a binary classification problem. That means, that the considered neighbors for a new tuple, only can be inliers or outliers. 
If the 

There are two operations calculated, when new data is presented to the model:
\citep[p.1256 pp.]{9065747}

$K$ is the number of neighbors that would be considered for classifying a new unlabled tuple. 
Even though this classifier is simple, the value of ‘K’ plays an important role in classifying the unlabeled data.
$K$ is an hyperparameter, that means it is an external configuration variable that need be determined befor the process begins.
It is task of hyperparamatic tuning to find a optimal number of $k$. 
You have to consider a bias-variance-trade of when you choose the optimal $K$. 
The smaller $K$ is, the more likley it is that the model suffers from overfitting, leading to higher variance and lower bias.
The resulting decision boundaries will be complex and fragmented.
The lager $K$ ist, the more likley it is, that the model suffers from underfitting, leading to smaller variance and lagrer bias.
The resulting decision boundaries maybe missing local patterns.
It is advised to choose a odd-number $K$, otherwiseat at an even classification the model could fail.  

Cross-validation is often been used to hyperparamatic tuning.
Alternatives are: Elbow Method, 

To determine the nearest neighbor, you need to calculate the distance between the new tuple and the neighbors. 
In classical k-NN, you use the Euclidean distance. 
By using the Euclidean-distance , k-NN is not affine equivariant.
Non-uniforly changes in the scale changes the distance landscap and thus leading to different set of nearest neighbors.
Resulting in different classifications. 

k-NN has many advantages. Its results are easy to understand and interpret.
The algorithm stays efficient with large traning sets eventhough it is invfluence by the curse of dimensional
There is no need for complex mathematical calculations, making it user-friendly.
Its is a non-paramatic model, that does not require any assumptions about the underlying distribution od the dataset.
That makes it flexible for complex datasets.

Some disadvantage of k-NN might be: the computational complexity. 
For a classification the model requires to store the entire dataset. Especially for large datasets, that could lead to slow prediction time. 
Moreover, storing the entire dataset is memory-intensive.
Because of the dependence of distance measurments, noisy data can influence the accurarcy negatively.
Chosing $K$ is vital for the models results. Through hyperparamatric tuning, one has to optimize the model. 





\subsection{Contrastive Learning (Intercont)}


\newpage

The Ordinary Least Squared was being critized because of its lack of robusteness. Just one outlier could have a large negative effect.

Mahalanobis disctance cant detect multiple outliers because of the masking effect, by wich multiple outliers dont have necesssarly a large mahananobis disctance (R99 S.212)
Main idea of MCD find h observations out of n whose classical covariance matrix has the lowest determinant. The MCD estimate is then rhe average of these h points 
and the MCD estimate of scatter is their covariance matrix. 

Procedure: 1. Creating initial subset. 
2 Possibillities: draw a random h subset H1, or draw a random (p+1)-subset auf J, and then compute 
technical terms:

Determinant - Sie ist das Produkt der Hauptdiagonalen minus dem Produkt der Nebendiagonalen. Das ist stimmt bis zu einer 3x3 Matrix, darüberhinaus werden andere Methoden verwendet (Laplace'scher Entwicklungssatz)
Es ist eine Eigenschaft die Auskunft über die Volumenverzerrung und Inventierbarkeit einer Matrix gibt. 
Inventierbar ist eine Matrix, wenn det (A) =/ 0. 
Geometrische Bedeutung:  Der Betrag der Determinante ist gleich dem Faktor, um den sich das Volumen (oder Fläche)  eines Einheitswürfels nach der Transformation vergrößert oder verkleinert.
Das Vorzeichen der det(A) gibt Auskunft über die Orientierung. det (A) > 0 bleibt Orientierung erhalten (zb Drehung oder Streckung)
Wenn det(A) < 0 wird die Orientierung umgekehrt (z.B. eine Spiegelung)

asymptotisch normalverteilt 

\newpage 
\newpage

Minimum Coavariance Determinant is asymptotically normal and robust. But it is hard to compute.


Therefor the authors constructed an algorithm, thats much faster than the first algorithm.
MCD Alternative: for dataset with very few observations/ excessive noise use Minimum Regularized Covariance Determinant
Insert: the curse of dimensionality

For the end: how can I be sure, that my results are legit?

for dataset analysis: multicolinearityt?



What is the solution?
The Minimum Covariance Determinant, first published in 1985 by , is a robust estimator of the covariance matrix in multidimensional dataset.
Given a dataset of n observations and a predetermined subset sie h, the MCD searches for the h observations whose empirical covariance matrix has the smalltest possible determinant.
A determinant is a mathematical property of a matrix.

How does it work?


What are the advantages?

What are the disadvantages?