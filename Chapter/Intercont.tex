\subsubsection*{Contrastive Learning (Intercont)}

\todo{insert whats about and how will you write this passage}
\textcolor{green}{procedure}
explain what contrastive learing is.
1 whats a Neural network: whats are the differences between deep learning and statistical models?
    a General informations
        where do we use it 
        what are these tasks
        why do we use them
        Types of NN
        Feet forward vs feet backward networks
        supervised and unsupervised learning
        advantages disadvantages 
        Differences between statistics and deep learning (k-NN defined as Machine Learning)

    b Details: 
        Arcitecture: Layers, defining, how many should you consider,
        Nodes, defining, which number to choose,
        Weights, definngm what happens to them
        Bias, defining, why do you need bias
        Activtion function, defining, hat type of af are there, differences
        Loss function, defining, differences, 
        Batch
        learning rate

2 Contrastive learning: A specially used Neural Network - specific
    a General informations  
        what is it
        where do we use it 
        why do we use it
        advantages and disadvantages
        loss functions 
    b Details   
        Use the specific details used in paper
        how is the 
\textcolor{red}{Neural Networks}

A neural network is a model inspired by the structure and function of the human brain. 
It is suitable for a lot of tasks: for example classification, clustering, pattern recognition and predictive tasks.
\todo{insert example papers maybe in the financial domain?}


The arcitecture of a neural network consists of layers. Those layers consist of nodes, wich are mathematical functions.
The first layer is the input layer. It receives the input data. \colorbox{red}{Each node in the input layer corrensponds to a feature in the data.}
After performing the first computation through the node, the result is pushed to the next layer, called hidden layer.
The network can be made up of aribrary number of hidden layers. 
Between layers there are the weights, biases and activation functions. 
The weight determines how important the connection between these two nodes are. The more imporant it is, the higher the weight. 
The bias is a threshold value that determines if the node should pass the result to the next layer.
If the threshold is met, the node is passing the result to the next layer.
The activation function is used to bring non-linearity into the model.
\todo{insert Leaky RELU, its slope coefficient and Tanh activation functions}  
Without an activation function, the model just ant predict linear results, because a linear function of a linear function is a linear function.
The model passes the input and multiplies it with the weight and adds a bias. 
The result is def into the activation function. If the function is met, the activation function passes the result onto another layer.
The last layer is called an output layer. 
The resulting output is compared to the known actual result, through a loss function.
After calculating the loss, the model uses backpropagation to send the result backwards thought the layers.  
By using the chain rule, the model calculates how much each weight and bias contributes to the loss, called gradient.
An optimizer is been used to adjust the bias and weights. The goal is to minimize the loss function.
The loss is calculated again, fulfilling an epoch. 
Models that use more than two hidden layers are often referred to as "deep networks".
\todo{insert: deep learning, differences, advantages and disadvantages}
Models that use this iteration process are called feedforward neural neutworks. 
The calculation are pushed in just one direction.
A different type of neural networks are the so called feedback neural networks. 
\todo{insert: feedbackward network, use cases disadvantages and advantages}
\todo{insert Diagram}




The neural network has a training stage, in wich it learns the right weights and it been fed with labled data.  


\textcolor{red}{Contrastive learning}
he idea of contrastive learning has emerged in computer vision.
In early research they tried to link a phrase with the right image and therefor needed to classification.
The problem arised in applying supervied learning, where human labour is needed to label the data, because there is potentially a very large number of classes.
Thus reducing the scalabillity.
Introducing metric learning, where the goal is to generate a numerical representation of the input, sa a image, and putting it into an embedding space resulting in a vector.
This initial vector is called anchor. New images are added and the network learns an embedding space, where simmiliar images are put cloder together and dissimiliar 
images are put further away. When the network is presented with a slightly different version of the anchor image, the network is capable of recognizing the most important 
aspects of this image, because it puts the new image near the anchor in the embedding space.  
Those networks reduce complexity by reducing the dimensionality of the input.
Supervised contrastive learning (SCL) is a branch that uses labeled data to train models explicitly to differentiate between
similar and dissimilar instances. In SCL, the model is trained on pairs of data points and their labels, indicating whether 
the data points are similar or dissimilar. The objective is to learn a representation space where similar instances are clustered 
closer together, while dissimilar instances are pushed apart (https://encord.com/blog/guide-to-contrastive-learning/)


The methods uses quite similar networks. F has two hidden layers.
The first one has u Nodes (units) and it uses the Tanh activation function.
The second layer has 2u Nodes and uses the LeakyReLu activation function.
G is similar but has u/4 and u/ Nodes bescause of smaller input sizes.
In  all layers but the first layer of F the slope coefficient is set to 0.2
The result are applied to a Batch Normalization so it re-centers and rescales the data for faster, more stable and less sensitive to hoe you initialize weights
\todo{insert details how its done and why}


\textcolor{green}{
The input data of the InterCont Method is supposed to be of tabular data.
The training set contains n in-class samples x. All of them are vectors of d dimensions.
The goal is to design a score that maps samples from the sample domain to a low vaulue if they are sampled from the underlying sample distribution from wich S is sampled.
If the sample is not from the underlying distribution it gets high score, thus is more likely a outlier. 
First a subsample is taken of k consecutive variables and m = d + 1 - k pairs %(a,b)% are generated.
Whereas k determines the number of variables that are considered and there cant exceed d dimensions.
a is the vector including all variables k and b is the vector of the rest d - k. 
The pairs are complementing each other. 
The method learns mapping yb using two networks F,G. Each network facilitates a or b.
The mutual information is been maximized by using the noise contrastive estimation framework of \citep{oord2018representation}.
In this framwork b is used as a query and the complementary a vector is used as a positive sample. Otherwise it is a negative sample.
Contrestive learning minimizes the distance bewteen query and positive sample and maximizes the distance bewteen query and negative sample.
Thats calles a masking effect, because 
}

\textcolor{red}{Contrastive Loss function
We need a performance measure to quantify the goodness of the learner, hence a loss function. 
It is a function of the true targets and the learned predictions.
The goal is to minimize the the loss, so that the model's predictions are more accurate.
But a perfectly fitting model can lead to the problem of overfittig.
There are plentyful of different options to choose from.
The most common loss functions are Mean Squared Error (MSE), Mean Absolute Error (MAE) and Huber Loss.
The Mean Squared Error is defined as: 
\begin{equation}
    \text{MSE} = \frac{1}{n} \sum_{i=1}^{n} (Y_i - \hat{Y}_i)^2
\end{equation}
The sum is squared because the difference between prediction and true target is defined as non-negative.
The MSE penalizes large prediction errors more heavily.
It is equivalent to the OLS objective function and is consistent with the normality assumption.
The Mean Absolute Error is defined as: 
\begin{equation}
    \text{MAE} = \frac{1}{n} \sum_{i=1}^{n} |Y_i - \hat{Y}_i|
\end{equation}
It uses the median instead of the mean for a more robust loss.
The Huber loss is a combination of MSE and MAE to robustify MSE and id defined as:
\begin{equation}
    L_{\delta}(y, \hat{y}) =
    \begin{cases}
    \frac{1}{2}(y - \hat{y})^2 & \text{for } |y - \hat{y}| \le \delta \\
    \delta |y - \hat{y}| - \frac{1}{2}\delta^2 & \text{otherwise}
    \end{cases}
\end{equation}
}

\todo{insert contrastive loss function of paper}
\todo{insert double normalization}

\textcolor{red}{Encoder network:
The encoder network takes the augmented instances as input and maps them to a latent representation space,
where meaningful features and similarities are captured. 
A latent space is a abstract, compressed and low-dimensional vector space of the input data, where essential features and underlying patterns are encoded.
The goal is to filter out noise and redundant information. 
It perserves semantic relationsships. Data points that are similar in the original, will be mapped to points that are close together. 
Points that are dissimilar will be far apart.
The latent space is produced by representation learning. 
Autoencoders are the most common way to learn a latent space. They consists of two main parts:
The encoder takes the high dimensional input and maps it to a much smaller vector in the latent space. This enforces the necessary compression.
The middle layer of the autoencoder is the bottleneck, wich contains the ompressed, low dimensional representation. 
The Decoder takes the compressed vector and attemps to reconstruct the original high dimensional input. 
The autoencoder is trained to minimize the difference between the original input and the reconstructed output. By forcing the information through the bottleneck, the 
enoder learns only the most essential features required for accurate reconstruction.
}
\textcolor{green}{Encoder Network}
To maximize the mutual information gain, the Intercont methods uses an encoder network based on noise contrastive estimation. \citep{oord2018representation}




\textcolor{green}{In Paper}
The contemporaneous NeuTraL AD work by Qiu et al. (2021) employs per-sample contrastive loss
for identifying anomalies in tabular data, similar to our work. However, there are crucial differences:
(1) NeuTraL AD learns specific masks, while we apply the entire set of the masks specified by a
window size k. (2) The role of NeuTraL AD masks is to mask-out parts that are irrelevant for specific
classes. In our case, we perform a two sided matching that identifies the masked part form the original.
(3) NeuTraL AD learns a single feature extractor (“encoder”) for both the original and transformed
data. In our case, the two sides of the contrastive loss are of very different dimensions $d - k$ and $k$
and we employ two different encoders. \cite[p.3]{shenkar2022anomaly}



\textcolor{red}{Projection Network 
A projection network is employed to refine the learned representations further. 
The projection network takes the output of the encoder network and projects it onto a lower-dimensional space, 
often referred to as the projection or embedding space. 
This additional projection step helps enhance the discriminative power of the learned representations. 
By mapping the representations to a lower-dimensional space, the projection network reduces the complexity and redundancy 
in the data, facilitating better separation between similar and dissimilar instances.(https://encord.com/blog/guide-to-contrastive-learning/)
}


\textcolor{red}{Contrastive Learning
Once the augmented instances are encoded and projected into the embedding space, the contrastive learning objective is applied. 
The objective is to maximize the agreement between positive pairs (instances from the same sample) and minimize the agreement between negative pairs (instances from different samples).
This encourages the model to pull similar instances closer together while pushing dissimilar instances apart. 
The similarity between instances is typically measured using a distance metric, such as Euclidean distance or cosine similarity. 
The model is trained to minimize the distance between positive pairs and maximize the distance between negative pairs 
in the embedding space. (https://encord.com/blog/guide-to-contrastive-learning/)
}



\textcolor{green}{
Example   
}




Contrastive Learning
latent space
embedding size / embedding space 
temperatur constant of the loss 
mutual information
L2 Norm 
cross entropy loss Adam optimzier 
LeakyRELU activation 
bagging efferct
slope coefficient 
tanh activation 
Batch normalization 